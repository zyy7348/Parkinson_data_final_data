---
title: "p8105_hw3_yz3300.Rmd"
author: "Yeyi Zhang"
date: "2017/10/7"
output:
  html_document:
    toc: true
    toc_float: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```


```{r libraries}
library(tidyverse)
library(haven)
library(janitor)
library(knitr)
library(reshape)
library(ggridges)
```

### Problem 1

First we read and clean the pulse dataset, omit observations for which BDI score wasn’t measured.

```{r clean_pulse}
pulse_data = read_sas("../data/public_pulse_data.sas7bdat") %>%
  clean_names() %>% 
  filter(!is.na(bdiscore_01m) & !is.na(bdiscore_06m) & !is.na(bdiscore_12m)) 

str(pulse_data)
```

Summarise: this dataset has `r nrow(pulse_data)` observations and `r ncol(pulse_data)` variables. Other than `id`, all variables are class `character`. `id` is the only numeric class variable.

* How many subjects are included? Make a table showing the number of subjects with observations at 1, 2, 3, or 4 visits.

There are `r length(unique(pulse_data$id))` subjects included.

First, we tidy the data,
```{r pulse_tidy_data}
pulse_tidy_data = pulse_data %>% 
  gather(key = "visit", value = "bdi_score", bdiscore_bl:bdiscore_12m) %>%
  separate(visit, into = c("remove", "visit"), sep = "_") %>%
  select(id, visit, everything(), -remove) %>%
  mutate(visit = replace(visit, visit == "bl", "1"),
         visit = replace(visit, visit == "01m", "2"),
         visit = replace(visit, visit == "06m", "3"),
         visit = replace(visit, visit == "12m", "4")) %>% 
  arrange(id, visit)
```

Then we make the table

```{r table_num_subj}
pulse_tidy_data %>%
  group_by(visit) %>% 
  summarise(num_subj = n()) %>% 
  kable()
```

* Make a table showing showing the mean, median, and standard deviaion of the BDI score at each visit.

```{r table_mean_med_sd}
pulse_tidy_data %>%  
  group_by(visit) %>% 
  summarise(mean_bid = mean(bdi_score), median_bdi = median(bdi_score), sd_bdi = sd(bdi_score)) %>%
  kable()
```

* Make box and violin plots showing the distribution of BDI score at each visit. Comment on the distribution of BDI score.

```{r box_plot_distri_bdiscore}
pulse_tidy_data %>% 
  ggplot(aes(x = visit, y = bdi_score)) + 
  geom_boxplot() +
  labs(
    y = "BDI score",
    title = "Box plot of BDI score"
  )
```

```{r violin_plot_distri_bdiscore}
pulse_tidy_data %>% 
  ggplot(aes(x = visit, y = bdi_score)) + 
  geom_violin(aes(fill = visit), color = "black", alpha = .5) + 
  stat_summary(fun.y = median, geom = "point", color = "black", size = 4) + 
  labs(
    y = "BDI score",
    title = "Violin plot of BDI score"
  ) +
  theme(legend.position = "bottom")
```

The distribution of BDI score at each visit seems quite alike, the first visit is different from others.

The distribution shows that the BDI scores stay almost around zero to 20, at very low level. There is only a few scores at higher levels. The median of the first visit is higher than the other three, where the other three are almost the same.

* Covert visit to a numeric variable, and make a “speghetti plot” of BDI score and comment on the stability.

```{r speghetti_plots_bdiscore}
set.seed(1)
pulse_tidy_data %>% 
  mutate(visit = as.numeric(visit)) %>%
  filter(id %in% sample(pulse_data$id, size = 30, replace = FALSE)) %>% 
  group_by(id) %>% 
  ggplot(aes(x = visit, y = bdi_score)) + 
  geom_path(color = "dark blue", alpha = .3) +
  labs(
    y = "BDI score",
    title = "'Speghetti plot' of BDI score"
  ) 
```

In the scatterplot, we can see that the BDI score within a person changes during four visits. Subjects with high BDI scores do not remain high BDI scores at 12 months.

### Problem 2

First, read and clean the Instacart data.

```{r clean_instac}
instacart = read_csv("../data/instacart_train_data.csv.zip") 
```

* Produce a table showing how many items are ordered in each department; limit the table to the seven departments from which the most items are ordered.

```{r table_item_order}
instacart %>% 
  select(department) %>% 
  group_by(department) %>% 
  summarise(num = n()) %>% 
  arrange(desc(num)) %>%
  kable()

instacart %>% 
  select(department) %>% 
  group_by(department) %>% 
  summarise(num = n()) %>% 
  arrange(desc(num)) %>%
  slice(1:7) %>% 
  kable()
```

* Make a table showing the most popular item in each department.

```{r most_pop}
instacart %>% 
  group_by(department) %>% 
  filter(!is.na(product_name)) %>% 
  summarise(most_pop_item = max(product_name))  %>% 
  kable()
```

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers

```{r table_mean_hod_PLA_CIC}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select(product_name, order_hour_of_day, order_dow) %>% 
  group_by(order_dow, product_name) %>% 
  summarise(mean_order_hod = mean(order_hour_of_day)) %>% 
  cast(product_name~order_dow) %>% 
  kable()
```

* Using violin and ridge plots, show the distribution of the order hour of the day for each department. Organize your plot according to the IQR of the order hour. Comment on the distributions, taking into account the which departments have the widest and narrowest IQRs.

```{r violin_plot_order_hod}
instacart %>% 
  select(department, order_hour_of_day) %>%
  group_by(department) %>%
  mutate(IQR = IQR(order_hour_of_day)) %>%
  ungroup() %>% 
  mutate(name = forcats::fct_reorder(department, IQR)) %>% 
  ggplot(aes(x = name, y = order_hour_of_day)) + 
  geom_violin(alpha = .5) +  
  stat_summary(fun.y = median, geom = "point", color = "black", size = 3) +
  geom_boxplot(width = 0.1) +
  labs(
    y = "IQR of order hour of day",
    title = "Violin plot of order hour of day"
  )
```

```{r ridge_plot_order_hod}
instacart %>%
  select(department, order_hour_of_day) %>%
  group_by(department) %>%
  mutate(IQR = IQR(order_hour_of_day)) %>%
  ungroup() %>% 
  mutate(name = forcats::fct_reorder(department, IQR)) %>% 
  ggplot(aes(x = order_hour_of_day, y = name)) + 
  geom_density_ridges(scale = .85) +
  labs(
    x = "Hour of day",
    title = "Ridge plot of order hour of day"
  ) 
```

```{r widest_and_narrowest}
range = instacart %>%
  select(department, order_hour_of_day) %>%
  group_by(department) %>%
  mutate(IQR = IQR(order_hour_of_day)) 

max = max(range$IQR)
min = min(range$IQR)

widest_department = range %>% 
  select(department, IQR) %>% 
  group_by(department) %>% 
  filter(IQR == max)

narrowest_department = range %>% 
  select(department, IQR) %>% 
  group_by(department) %>% 
  filter(IQR == min) 
```

The distributions of two plots show that the order hours concentrate between seven to twenty in a day. `r unique(widest_department$department)` are departments have the widest IQRs and `r unique(narrowest_department$department)` are departments have the narrowest IQRs. 

### Problem 3

First read the dataset

```{r read_ny_noaa}
ny_noaa = read_csv("../data/nynoaadat.zip") %>% 
  rename(replace = c("id" = "station")) 

str(ny_noaa)
```

* How many observations are included?  
  * How many stations?

This dataset has `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. Other than `date`, all variables are class `character` or `numeric`.  Precipitation, snowfall and snow depth are integer class variables. There are `r length(unique(ny_noaa$station))` stations. 

  * How much missing data is there for tmax and snow? Does this vary by station?

There is `r sum(is.na(ny_noaa$tmax))` missing data for tmax and `r sum(is.na(ny_noaa$snow))` missing data for snow. 

Then we make two plots to see whether the missing data varies by station.

```{r plot_missing_tmax}
ny_noaa %>% 
  group_by(station) %>%
  select(station, tmax) %>%
  summarise(sum_tmax = sum(is.na(tmax))) %>%
  ggplot(aes(x = station, y = sum_tmax)) +
  geom_point(alpha = .5) +
  labs(
    title = "Missing data for tmax",
    y = "Numbers of missing data"
  ) 
```

```{r plot_missing_snow}
ny_noaa %>% 
  group_by(station) %>%
  select(station, snow) %>%
  summarise(sum_snow = sum(is.na(snow))) %>%
  ggplot(aes(x = station, y = sum_snow)) +
  geom_point(alpha = .5) +
  labs(
    title = "Missing data for snow",
    y = "Numbers of missing data"
  ) 
```

We can see from the two plots that missing data does vary by station. There are a bunch of stations have quite small sums of missing data for both tmax and snow, where some stations have large sums of missing data for both values.

* What is the year that contains the largest snowfall on a single day at any single weather station (i.e. what is the year containing the single largest snowfall anywhere in the state in the 30 years of monitoring). Can you find information online that supports your finding?


```{r snowfall}
largest_snow = ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  filter(!is.na(snow)) %>% 
  arrange(desc(snow)) %>% 
  slice(1) 
```

`r select(largest_snow, year)` is the year that contains the largest snowfall on `r select(largest_snow, month)` / `r select(largest_snow, day)` at `r  select(largest_snow, station)` station. Cannot find informaiton that supports my findings.

* Limiting your data to observations with snowfall values greater than 0 and less than 100, make a ridge plot showing the distribution of snowfall values for each year. Comment on the recorded snowfall values – are they clustered around specific entries? If so, why?

We limit data.

```{r limit_ny_noaa}
limit_noaa = ny_noaa %>%
  filter(!is.na(snow),
         snow > 0 & snow < 100) %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") 
```

Make a ridge plot showing the distribution.

```{r ridge_distri_snowfall}
limit_noaa %>% 
  ggplot(aes(x = snow, y = year)) + 
  geom_density_ridges(scale = .7) +
  labs(
    title = "Ridge plot of snowfall",
    x = "Snowfall(mm)"
  ) 
```

The recorded snowfall values clustered around specific entries because the snowfalls between 0-100mm are not extreme values, which means that the snowfalls are constant each year.

* Make a useful plot showing tmax against tmin. 

```{r tmax_against_tmin}
ny_noaa %>% 
  filter(!is.na(tmin), !is.na(tmax)) %>% 
  mutate(tmin = as.numeric(tmin), 
         tmax = as.numeric(tmax)) %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  group_by(year, month) %>% 
  summarise(mean_tmax = mean(tmax), mean_tmin = mean(tmin)) %>% 
  ggplot(aes(x = mean_tmin, y = mean_tmax)) +
  geom_point(alpha = .5) +
  geom_smooth(size = 1) +
   labs(
    title = "Plot of temperature",
    x = "Mean of minimum daily temperature(tenths of degrees C)",
    y = "Mean of maximum daily temperature(tenths of degrees C)"
  ) +
  theme(legend.position = "bottom")
```

* Separate the date variable into year, month, and day variables. For each station and month, average across year to obtain the station-specific monthly average tmax. Make a spaghetti plot showing the average tmax curve for each station. Comment on your plot.

First, spearate the data.

```{r separate}
spaghetti_plot = ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  select(station, year, month, tmax) %>% 
  filter(!is.na(tmax)) %>%  
  group_by(month, station, year) %>%
  summarise(mean_tmax = mean(as.numeric(tmax))) 
```

Make a spaghetti plot. 

```{r spaghetti_plot}
spaghetti_plot %>%  
  ggplot(aes(x = month, y = mean_tmax, group = station)) + 
  geom_path(color = "blue", alpha = .5) +
    labs(
    title = "Spaghetti plot of average maximum temperature",
    y = "Mean of maximum temperature(tenths of degrees C)"
  ) 
```

The spaghetti plot shows a common sense that in the middle of the year, when it is summer, we have the higher temperature, and in winter, we have mean of maximum temperature at lower level.